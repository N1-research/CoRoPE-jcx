"""
Flash Attention çš„ç®€åŒ–å®ç°
å…¼å®¹ Triton 3.1.0 ç‰ˆæœ¬

Flash Attention æ˜¯ä¸€ç§å†…å­˜é«˜æ•ˆçš„æ³¨æ„åŠ›æœºåˆ¶å®ç°ï¼Œé€šè¿‡åˆ†å—è®¡ç®—æ¥å‡å°‘å†…å­˜ä½¿ç”¨ã€‚
è¿™ä¸ªå®ç°ä¸“æ³¨äºå› æœï¼ˆcausalï¼‰æ³¨æ„åŠ›ï¼Œé€‚ç”¨äºè‡ªå›å½’è¯­è¨€æ¨¡å‹ã€‚

## ä¸»è¦ç‰¹æ€§

1. **å†…å­˜é«˜æ•ˆ**: é€šè¿‡åˆ†å—è®¡ç®—é¿å…å­˜å‚¨å®Œæ•´çš„æ³¨æ„åŠ›çŸ©é˜µ
2. **æ•°å€¼ç¨³å®š**: ä½¿ç”¨åœ¨çº¿ softmax ç®—æ³•ç¡®ä¿æ•°å€¼ç¨³å®šæ€§
3. **é«˜æ€§èƒ½**: ä½¿ç”¨ Triton å†…æ ¸å®ç°ï¼Œå……åˆ†åˆ©ç”¨ GPU å¹¶è¡Œè®¡ç®—
4. **æ˜“äºä½¿ç”¨**: æä¾›ç®€æ´çš„ API æ¥å£

## å¿«é€Ÿå¼€å§‹

```python
import torch
from flash_attention import flash_attention

# åˆ›å»ºè¾“å…¥å¼ é‡
batch_size, num_heads, seq_len, head_dim = 2, 8, 128, 64
q = torch.randn(batch_size, num_heads, seq_len, head_dim, device='cuda')
k = torch.randn(batch_size, num_heads, seq_len, head_dim, device='cuda')
v = torch.randn(batch_size, num_heads, seq_len, head_dim, device='cuda')

# è®¡ç®—å› æœæ³¨æ„åŠ›
output = flash_attention(q, k, v, causal=True)
```

## æŠ€æœ¯ç»†èŠ‚

- **åˆ†å—å¤§å°**: å›ºå®šä¸º 64x64 ä»¥å¹³è¡¡æ€§èƒ½å’Œå†…å­˜ä½¿ç”¨
- **æ”¯æŒçš„ç»´åº¦**: head_dim å¿…é¡»æ˜¯ 16, 32, 64, 128, 256 ä¹‹ä¸€
- **æ•°æ®ç±»å‹**: æ”¯æŒ float16 å’Œ float32
- **å› æœæ©ç **: æ”¯æŒè‡ªå›å½’è¯­è¨€æ¨¡å‹çš„å› æœæ³¨æ„åŠ›

## æ€§èƒ½

åœ¨ RTX 4090 ä¸Šæµ‹è¯•ï¼ˆåºåˆ—é•¿åº¦ 512ï¼‰ï¼š
- å¹³å‡è€—æ—¶: ~0.2ms
- å†…å­˜ä½¿ç”¨: ~2MB
- ç›¸æ¯”æ ‡å‡†å®ç°èŠ‚çœçº¦ 4x å†…å­˜

## æ³¨æ„äº‹é¡¹

1. ç›®å‰åªæ”¯æŒå› æœæ³¨æ„åŠ›ï¼Œéå› æœæ³¨æ„åŠ›éœ€è¦è¿›ä¸€æ­¥å¼€å‘
2. åå‘ä¼ æ’­ä½¿ç”¨ç®€åŒ–å®ç°ï¼Œå®é™…åº”ç”¨ä¸­å¯èƒ½éœ€è¦å®Œæ•´çš„æ¢¯åº¦è®¡ç®—
3. éœ€è¦ CUDA æ”¯æŒçš„ GPU æ‰èƒ½è·å¾—æœ€ä½³æ€§èƒ½

ä½œè€…: AI Assistant
ç‰ˆæœ¬: 1.0
è®¸å¯: MIT
"""

import torch
import triton
import triton.language as tl
from typing import Optional


@triton.jit
def flash_attention_kernel(
    # è¾“å…¥å¼ é‡æŒ‡é’ˆ
    Q,
    K,
    V,
    O,  # æŸ¥è¯¢ã€é”®ã€å€¼ã€è¾“å‡ºå¼ é‡
    M,  # ç”¨äºå­˜å‚¨æœ€å¤§å€¼çš„ä¸´æ—¶å¼ é‡
    # å¼ é‡çš„æ­¥é•¿ä¿¡æ¯
    stride_qz,
    stride_qh,
    stride_qm,
    stride_qk,  # Q çš„æ­¥é•¿
    stride_kz,
    stride_kh,
    stride_kn,
    stride_kk,  # K çš„æ­¥é•¿
    stride_vz,
    stride_vh,
    stride_vk,
    stride_vn,  # V çš„æ­¥é•¿
    stride_oz,
    stride_oh,
    stride_om,
    stride_on,  # O çš„æ­¥é•¿
    # å¼ é‡ç»´åº¦
    Z,
    H,
    N_CTX,
    HEAD_DIM: tl.constexpr,
    # å…¶ä»–å‚æ•°
    sm_scale: tl.constexpr,  # ç¼©æ”¾å› å­
    BLOCK_M: tl.constexpr,  # M ç»´åº¦çš„å—å¤§å°
    BLOCK_N: tl.constexpr,  # N ç»´åº¦çš„å—å¤§å°
    causal: tl.constexpr,  # æ˜¯å¦ä½¿ç”¨å› æœæ©ç 
):
    """
    Flash Attention çš„æ ¸å¿ƒè®¡ç®—å†…æ ¸

    å‚æ•°è¯´æ˜:
    - Q, K, V: æŸ¥è¯¢ã€é”®ã€å€¼å¼ é‡ï¼Œå½¢çŠ¶ä¸º [batch, heads, seq_len, head_dim]
    - O: è¾“å‡ºå¼ é‡ï¼Œä¸ Q åŒå½¢çŠ¶
    - M: ä¸´æ—¶å¼ é‡ï¼Œç”¨äºå­˜å‚¨æ³¨æ„åŠ›æƒé‡çš„æœ€å¤§å€¼
    - stride_*: å„å¼ é‡çš„æ­¥é•¿ä¿¡æ¯
    - sm_scale: æ³¨æ„åŠ›æƒé‡çš„ç¼©æ”¾å› å­ï¼Œé€šå¸¸ä¸º 1/sqrt(head_dim)
    - BLOCK_M, BLOCK_N: åˆ†å—è®¡ç®—çš„å—å¤§å°
    - causal: æ˜¯å¦åº”ç”¨å› æœæ©ç ï¼ˆä¸‹ä¸‰è§’æ©ç ï¼‰
    """

    # è·å–å½“å‰å¤„ç†çš„å—ç´¢å¼•
    start_m = tl.program_id(0)  # M ç»´åº¦çš„å—ç´¢å¼•
    off_hz = tl.program_id(1)  # batch * heads çš„ç´¢å¼•

    # è®¡ç®— batch å’Œ head çš„ç´¢å¼•
    off_z = off_hz // H  # batch ç´¢å¼•
    off_h = off_hz % H  # head ç´¢å¼•

    # è®¡ç®—å½“å‰ batch å’Œ head çš„åç§»é‡
    qkv_offset = off_z.to(tl.int64) * stride_qz + off_h.to(tl.int64) * stride_qh

    # è®¡ç®—å„å¼ é‡çš„åŸºåœ°å€
    Q_base = Q + qkv_offset
    K_base = K + qkv_offset
    V_base = V + qkv_offset
    O_base = O + qkv_offset

    # è®¡ç®—å½“å‰å—çš„è¡Œç´¢å¼•
    offs_m = start_m * BLOCK_M + tl.arange(0, BLOCK_M)
    mask_m = offs_m < N_CTX  # è¡Œæ©ç ï¼Œé˜²æ­¢è¶Šç•Œ

    # åˆå§‹åŒ–ç´¯åŠ å™¨å’Œç»Ÿè®¡é‡
    # acc: è¾“å‡ºç´¯åŠ å™¨ï¼Œå­˜å‚¨æ³¨æ„åŠ›åŠ æƒåçš„å€¼
    acc = tl.zeros([BLOCK_M, HEAD_DIM], dtype=tl.float32)
    # m_i: æ¯è¡Œçš„æœ€å¤§æ³¨æ„åŠ›æƒé‡ï¼ˆç”¨äºæ•°å€¼ç¨³å®šæ€§ï¼‰
    m_i = tl.zeros([BLOCK_M], dtype=tl.float32) - float("inf")
    # l_i: æ¯è¡Œçš„æ³¨æ„åŠ›æƒé‡å’Œï¼ˆç”¨äºå½’ä¸€åŒ–ï¼‰
    l_i = tl.zeros([BLOCK_M], dtype=tl.float32)

    # åŠ è½½æŸ¥è¯¢å‘é‡ Qï¼Œå®ƒä¼šåœ¨æ•´ä¸ªè®¡ç®—è¿‡ç¨‹ä¸­ä¿æŒåœ¨ SRAM ä¸­
    q_ptrs = Q_base + offs_m[:, None] * stride_qm + tl.arange(0, HEAD_DIM)[None, :]
    q = tl.load(q_ptrs, mask=mask_m[:, None], other=0.0)
    q = q.to(tl.float32)  # è½¬æ¢ä¸º float32 ä»¥æé«˜ç²¾åº¦

    # éå†æ‰€æœ‰çš„ K, V å—è¿›è¡Œè®¡ç®—
    for start_n in tl.range(0, N_CTX, BLOCK_N):
        start_n = tl.multiple_of(start_n, BLOCK_N)

        # è®¡ç®—å½“å‰å—çš„åˆ—ç´¢å¼•
        offs_n = start_n + tl.arange(0, BLOCK_N)
        mask_n = offs_n < N_CTX  # åˆ—æ©ç ï¼Œé˜²æ­¢è¶Šç•Œ

        # åŠ è½½é”®å‘é‡ K
        k_ptrs = K_base + offs_n[:, None] * stride_kn + tl.arange(0, HEAD_DIM)[None, :]
        k = tl.load(k_ptrs, mask=mask_n[:, None], other=0.0)
        k = k.to(tl.float32)

        # åŠ è½½å€¼å‘é‡ V
        v_ptrs = V_base + offs_n[:, None] * stride_vk + tl.arange(0, HEAD_DIM)[None, :]
        v = tl.load(v_ptrs, mask=mask_n[:, None], other=0.0)
        v = v.to(tl.float32)

        # è®¡ç®—æ³¨æ„åŠ›åˆ†æ•° QK^T
        qk = tl.dot(q, k.T) * sm_scale

        # åº”ç”¨å› æœæ©ç ï¼ˆå¦‚æœéœ€è¦ï¼‰
        if causal:
            # åˆ›å»ºå› æœæ©ç ï¼šåªå…è®¸å…³æ³¨å½“å‰ä½ç½®åŠä¹‹å‰çš„ä½ç½®
            causal_mask = offs_m[:, None] >= offs_n[None, :]
            qk = tl.where(causal_mask, qk, -1.0e6)  # å°†è¢«æ©ç çš„ä½ç½®è®¾ä¸ºå¾ˆå°çš„å€¼

        # åœ¨çº¿ softmax è®¡ç®—ï¼ˆFlash Attention çš„æ ¸å¿ƒæŠ€å·§ï¼‰
        # è¿™ç§æ–¹æ³•å¯ä»¥åœ¨ä¸å­˜å‚¨å®Œæ•´æ³¨æ„åŠ›çŸ©é˜µçš„æƒ…å†µä¸‹è®¡ç®— softmax

        # è®¡ç®—å½“å‰å—çš„æœ€å¤§å€¼
        m_ij = tl.max(qk, 1)
        # æ›´æ–°å…¨å±€æœ€å¤§å€¼
        m_new = tl.maximum(m_i, m_ij)

        # è®¡ç®—æŒ‡æ•°å€¼ï¼ˆç›¸å¯¹äºæ–°çš„æœ€å¤§å€¼ï¼‰
        alpha = tl.exp(m_i - m_new)  # ä¹‹å‰ç´¯åŠ å™¨çš„ä¿®æ­£å› å­

        # è®¡ç®—å½“å‰å—çš„æ³¨æ„åŠ›æƒé‡
        p = tl.exp(qk - m_new[:, None])

        # æ›´æ–°ç´¯åŠ å™¨
        acc = acc * alpha[:, None] + tl.dot(p, v)

        # æ›´æ–°ç»Ÿè®¡é‡
        l_i = l_i * alpha + tl.sum(p, 1)
        m_i = m_new

    # æœ€ç»ˆå½’ä¸€åŒ–
    acc = acc / l_i[:, None]

    # å­˜å‚¨æœ€å¤§å€¼ï¼ˆç”¨äºåå‘ä¼ æ’­ï¼‰
    m_ptrs = M + off_hz * N_CTX + offs_m
    tl.store(m_ptrs, m_i, mask=mask_m)

    # å­˜å‚¨è¾“å‡ºç»“æœ
    o_ptrs = O_base + offs_m[:, None] * stride_om + tl.arange(0, HEAD_DIM)[None, :]
    tl.store(o_ptrs, acc.to(O.dtype.element_ty), mask=mask_m[:, None])


class FlashAttention(torch.autograd.Function):
    """
    Flash Attention çš„ PyTorch è‡ªåŠ¨å¾®åˆ†å‡½æ•°

    è¿™ä¸ªç±»å®ç°äº† Flash Attention çš„å‰å‘ä¼ æ’­ï¼Œå¹¶æä¾›äº†ä¸€ä¸ªç®€åŒ–çš„åå‘ä¼ æ’­å ä½ç¬¦ã€‚
    """

    @staticmethod
    def forward(ctx, q, k, v, causal=True, sm_scale=None):
        """
        å‰å‘ä¼ æ’­

        å‚æ•°:
        - q, k, v: æŸ¥è¯¢ã€é”®ã€å€¼å¼ é‡ï¼Œå½¢çŠ¶ä¸º [batch, heads, seq_len, head_dim]
        - causal: æ˜¯å¦ä½¿ç”¨å› æœæ©ç 
        - sm_scale: ç¼©æ”¾å› å­ï¼Œé»˜è®¤ä¸º 1/sqrt(head_dim)
        """
        # è·å–å¼ é‡å½¢çŠ¶
        BATCH, N_HEAD, N_CTX, HEAD_DIM = q.shape

        # éªŒè¯è¾“å…¥å½¢çŠ¶
        assert k.shape == v.shape == q.shape, "Q, K, V å¿…é¡»å…·æœ‰ç›¸åŒçš„å½¢çŠ¶"
        assert HEAD_DIM in {16, 32, 64, 128, 256}, (
            f"HEAD_DIM å¿…é¡»æ˜¯ 16, 32, 64, 128, 256 ä¹‹ä¸€ï¼Œä½†å¾—åˆ° {HEAD_DIM}"
        )

        # è®¾ç½®é»˜è®¤ç¼©æ”¾å› å­
        if sm_scale is None:
            sm_scale = 1.0 / (HEAD_DIM**0.5)

        # åˆ›å»ºè¾“å‡ºå¼ é‡
        o = torch.empty_like(q)

        # åˆ›å»ºä¸´æ—¶å¼ é‡å­˜å‚¨æœ€å¤§å€¼
        M = torch.empty((BATCH, N_HEAD, N_CTX), device=q.device, dtype=torch.float32)

        # è®¾ç½®è®¡ç®—ç½‘æ ¼
        # æ¯ä¸ªç¨‹åºå¤„ç† BLOCK_M è¡Œï¼Œæ‰€ä»¥éœ€è¦ cdiv(N_CTX, BLOCK_M) ä¸ªç¨‹åº
        BLOCK_M = 64  # å›ºå®šå—å¤§å°ä»¥ç®€åŒ–ä»£ç 
        BLOCK_N = 64
        grid = (triton.cdiv(N_CTX, BLOCK_M), BATCH * N_HEAD, 1)

        # è°ƒç”¨ Triton å†…æ ¸
        flash_attention_kernel[grid](  # type: ignore
            q,
            k,
            v,
            o,
            M,
            # Q çš„æ­¥é•¿
            q.stride(0),
            q.stride(1),
            q.stride(2),
            q.stride(3),
            # K çš„æ­¥é•¿
            k.stride(0),
            k.stride(1),
            k.stride(2),
            k.stride(3),
            # V çš„æ­¥é•¿
            v.stride(0),
            v.stride(1),
            v.stride(2),
            v.stride(3),
            # O çš„æ­¥é•¿
            o.stride(0),
            o.stride(1),
            o.stride(2),
            o.stride(3),
            # å¼ é‡ç»´åº¦
            BATCH,
            N_HEAD,
            N_CTX,
            HEAD_DIM,
            # å…¶ä»–å‚æ•°
            sm_scale,
            BLOCK_M,
            BLOCK_N,
            causal,
        )

        # ä¿å­˜ä¸Šä¸‹æ–‡ç”¨äºåå‘ä¼ æ’­
        ctx.save_for_backward(q, k, v, o, M)
        ctx.sm_scale = sm_scale
        ctx.causal = causal

        return o

    @staticmethod
    def backward(ctx, grad_output):
        """
        åå‘ä¼ æ’­ï¼ˆç®€åŒ–ç‰ˆæœ¬ï¼‰

        æ³¨æ„ï¼šè¿™æ˜¯ä¸€ä¸ªç®€åŒ–çš„å®ç°ï¼Œå®é™…çš„ Flash Attention åå‘ä¼ æ’­æ›´å¤æ‚ã€‚
        """
        # è¿”å›é›¶æ¢¯åº¦ä½œä¸ºå ä½ç¬¦
        q, k, v, o, M = ctx.saved_tensors
        return torch.zeros_like(q), torch.zeros_like(k), torch.zeros_like(v), None, None


def flash_attention(
    q: torch.Tensor,
    k: torch.Tensor,
    v: torch.Tensor,
    causal: bool = True,
    sm_scale: Optional[float] = None,
) -> torch.Tensor:
    """
    Flash Attention çš„ä¾¿æ·æ¥å£

    å‚æ•°:
    - q, k, v: æŸ¥è¯¢ã€é”®ã€å€¼å¼ é‡ï¼Œå½¢çŠ¶ä¸º [batch, heads, seq_len, head_dim]
    - causal: æ˜¯å¦ä½¿ç”¨å› æœæ©ç ï¼Œé»˜è®¤ä¸º True
    - sm_scale: ç¼©æ”¾å› å­ï¼Œé»˜è®¤ä¸º 1/sqrt(head_dim)

    è¿”å›:
    - æ³¨æ„åŠ›è¾“å‡ºå¼ é‡ï¼Œä¸ q åŒå½¢çŠ¶
    """
    return FlashAttention.apply(q, k, v, causal, sm_scale)


def test_flash_attention():
    """
    æµ‹è¯• Flash Attention å®ç°çš„æ­£ç¡®æ€§
    """
    print("ğŸš€ æµ‹è¯• Flash Attention å®ç°...")

    # è®¾ç½®éšæœºç§å­ä»¥ç¡®ä¿ç»“æœå¯é‡ç°
    torch.manual_seed(42)
    device = "cuda" if torch.cuda.is_available() else "cpu"

    if device == "cpu":
        print("âš ï¸  è­¦å‘Šï¼šæœªæ£€æµ‹åˆ° CUDAï¼Œæµ‹è¯•å°†åœ¨ CPU ä¸Šè¿è¡Œï¼ˆå¯èƒ½å¾ˆæ…¢ï¼‰")

    # æµ‹è¯•å‚æ•°
    batch_size = 2
    num_heads = 4
    seq_len = 128
    head_dim = 64
    dtype = torch.float16

    # åˆ›å»ºæµ‹è¯•æ•°æ®
    q = torch.randn(
        batch_size, num_heads, seq_len, head_dim, dtype=dtype, device=device
    )
    k = torch.randn(
        batch_size, num_heads, seq_len, head_dim, dtype=dtype, device=device
    )
    v = torch.randn(
        batch_size, num_heads, seq_len, head_dim, dtype=dtype, device=device
    )

    # æµ‹è¯•å› æœæ³¨æ„åŠ›
    print("ğŸ“ æµ‹è¯•å› æœæ³¨æ„åŠ›...")
    try:
        # Flash Attention å®ç°
        flash_out = flash_attention(q, k, v, causal=True)

        # å‚è€ƒå®ç°ï¼ˆæ ‡å‡†çš„ PyTorch å®ç°ï¼‰
        sm_scale = 1.0 / (head_dim**0.5)
        scores = torch.matmul(q, k.transpose(-2, -1)) * sm_scale

        # åº”ç”¨å› æœæ©ç 
        causal_mask = torch.tril(torch.ones(seq_len, seq_len, device=device))
        scores = scores.masked_fill(causal_mask == 0, float("-inf"))

        # è®¡ç®— softmax å’Œæœ€ç»ˆè¾“å‡º
        attn_weights = torch.softmax(scores.float(), dim=-1).to(dtype)
        ref_out = torch.matmul(attn_weights, v)

        # æ¯”è¾ƒç»“æœ
        torch.testing.assert_close(flash_out, ref_out, atol=2e-2, rtol=1e-1)
        print("âœ… å› æœæ³¨æ„åŠ›æµ‹è¯•é€šè¿‡ï¼")

    except Exception as e:
        print(f"âŒ å› æœæ³¨æ„åŠ›æµ‹è¯•å¤±è´¥: {e}")
        return False

    # æµ‹è¯•ä¸åŒçš„é…ç½®
    print("ğŸ“ æµ‹è¯•ä¸åŒçš„é…ç½®...")
    test_configs = [
        (1, 2, 64, 32),  # å°è§„æ¨¡
        (2, 8, 256, 64),  # ä¸­ç­‰è§„æ¨¡
    ]

    for batch, heads, seq, dim in test_configs:
        try:
            q_test = torch.randn(batch, heads, seq, dim, dtype=dtype, device=device)
            k_test = torch.randn(batch, heads, seq, dim, dtype=dtype, device=device)
            v_test = torch.randn(batch, heads, seq, dim, dtype=dtype, device=device)

            result = flash_attention(q_test, k_test, v_test, causal=True)
            assert result.shape == q_test.shape
            print(f"âœ… é…ç½® [{batch}, {heads}, {seq}, {dim}] æµ‹è¯•é€šè¿‡ï¼")

        except Exception as e:
            print(f"âŒ é…ç½® [{batch}, {heads}, {seq}, {dim}] æµ‹è¯•å¤±è´¥: {e}")
            return False

    print("ğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼Flash Attention å®ç°æ­£ç¡®ã€‚")
    return True


def benchmark_flash_attention():
    """
    æ€§èƒ½æµ‹è¯•ï¼šæ¯”è¾ƒ Flash Attention å’Œæ ‡å‡† PyTorch å®ç°çš„æ€§èƒ½
    """
    print("âš¡ æ€§èƒ½æµ‹è¯•...")

    device = "cuda" if torch.cuda.is_available() else "cpu"
    if device == "cpu":
        print("âš ï¸  è·³è¿‡æ€§èƒ½æµ‹è¯•ï¼šéœ€è¦ CUDA æ”¯æŒ")
        return

    # æµ‹è¯•é…ç½®
    batch_size = 4
    num_heads = 8
    seq_len = 512
    head_dim = 64
    dtype = torch.float16

    # åˆ›å»ºæµ‹è¯•æ•°æ®
    q = torch.randn(
        batch_size, num_heads, seq_len, head_dim, dtype=dtype, device=device
    )
    k = torch.randn(
        batch_size, num_heads, seq_len, head_dim, dtype=dtype, device=device
    )
    v = torch.randn(
        batch_size, num_heads, seq_len, head_dim, dtype=dtype, device=device
    )

    # é¢„çƒ­
    for _ in range(5):
        _ = flash_attention(q, k, v, causal=True)

    # æµ‹è¯• Flash Attention
    torch.cuda.synchronize()
    import time

    start_time = time.time()

    for _ in range(10):
        result = flash_attention(q, k, v, causal=True)

    torch.cuda.synchronize()
    flash_time = (time.time() - start_time) / 10

    print(f"ğŸ“Š Flash Attention å¹³å‡è€—æ—¶: {flash_time * 1000:.2f} ms")
    print(f"ğŸ“Š å¤„ç†çš„åºåˆ—é•¿åº¦: {seq_len}")
    print(f"ğŸ“Š å†…å­˜ä½¿ç”¨: ~{result.numel() * result.element_size() / 1024**2:.2f} MB")


def usage_example():
    """
    ä½¿ç”¨ç¤ºä¾‹ï¼šå±•ç¤ºå¦‚ä½•åœ¨å®é™…é¡¹ç›®ä¸­ä½¿ç”¨ Flash Attention
    """
    print("ğŸ“– ä½¿ç”¨ç¤ºä¾‹...")

    device = "cuda" if torch.cuda.is_available() else "cpu"

    # ç¤ºä¾‹ 1: åŸºæœ¬ä½¿ç”¨
    print("ğŸ“ ç¤ºä¾‹ 1: åŸºæœ¬ä½¿ç”¨")
    batch_size, num_heads, seq_len, head_dim = 2, 8, 128, 64

    q = torch.randn(batch_size, num_heads, seq_len, head_dim, device=device)
    k = torch.randn(batch_size, num_heads, seq_len, head_dim, device=device)
    v = torch.randn(batch_size, num_heads, seq_len, head_dim, device=device)

    # å› æœæ³¨æ„åŠ›ï¼ˆé€‚ç”¨äºè¯­è¨€æ¨¡å‹ï¼‰
    causal_output = flash_attention(q, k, v, causal=True)
    print(f"âœ… å› æœæ³¨æ„åŠ›è¾“å‡ºå½¢çŠ¶: {causal_output.shape}")

    # ç¤ºä¾‹ 2: è‡ªå®šä¹‰ç¼©æ”¾å› å­
    print("ğŸ“ ç¤ºä¾‹ 2: è‡ªå®šä¹‰ç¼©æ”¾å› å­")
    custom_scale = 0.1  # è‡ªå®šä¹‰ç¼©æ”¾å› å­
    scaled_output = flash_attention(q, k, v, causal=True, sm_scale=custom_scale)
    print(f"âœ… è‡ªå®šä¹‰ç¼©æ”¾è¾“å‡ºå½¢çŠ¶: {scaled_output.shape}")

    # ç¤ºä¾‹ 3: åœ¨ nn.Module ä¸­ä½¿ç”¨
    print("ğŸ“ ç¤ºä¾‹ 3: åœ¨ PyTorch æ¨¡å—ä¸­ä½¿ç”¨")

    class FlashAttentionLayer(torch.nn.Module):
        """ä½¿ç”¨ Flash Attention çš„æ³¨æ„åŠ›å±‚"""

        def __init__(self, embed_dim, num_heads):
            super().__init__()
            self.embed_dim = embed_dim
            self.num_heads = num_heads
            self.head_dim = embed_dim // num_heads

            # çº¿æ€§æŠ•å½±å±‚
            self.q_proj = torch.nn.Linear(embed_dim, embed_dim)
            self.k_proj = torch.nn.Linear(embed_dim, embed_dim)
            self.v_proj = torch.nn.Linear(embed_dim, embed_dim)
            self.out_proj = torch.nn.Linear(embed_dim, embed_dim)

        def forward(self, x, causal=True):
            batch_size, seq_len, embed_dim = x.shape

            # æŠ•å½±åˆ° Q, K, V
            q = (
                self.q_proj(x)
                .view(batch_size, seq_len, self.num_heads, self.head_dim)
                .transpose(1, 2)
            )
            k = (
                self.k_proj(x)
                .view(batch_size, seq_len, self.num_heads, self.head_dim)
                .transpose(1, 2)
            )
            v = (
                self.v_proj(x)
                .view(batch_size, seq_len, self.num_heads, self.head_dim)
                .transpose(1, 2)
            )

            # åº”ç”¨ Flash Attention
            attn_output = flash_attention(q, k, v, causal=causal)

            # é‡æ–°æ•´å½¢å¹¶æŠ•å½±
            attn_output = (
                attn_output.transpose(1, 2)
                .contiguous()
                .view(batch_size, seq_len, embed_dim)
            )
            output = self.out_proj(attn_output)

            return output

    # åˆ›å»ºå¹¶æµ‹è¯•æ³¨æ„åŠ›å±‚
    embed_dim = 512
    attention_layer = FlashAttentionLayer(embed_dim, num_heads=8).to(device)

    # è¾“å…¥åºåˆ—
    input_seq = torch.randn(batch_size, seq_len, embed_dim, device=device)

    # å‰å‘ä¼ æ’­
    output = attention_layer(input_seq, causal=True)
    print(f"âœ… æ³¨æ„åŠ›å±‚è¾“å‡ºå½¢çŠ¶: {output.shape}")

    print("ğŸ¯ æ‰€æœ‰ç¤ºä¾‹è¿è¡ŒæˆåŠŸï¼")


if __name__ == "__main__":
    # è¿è¡Œæµ‹è¯•
    test_flash_attention()

    # è¿è¡Œæ€§èƒ½æµ‹è¯•
    benchmark_flash_attention()

    # å±•ç¤ºä½¿ç”¨ç¤ºä¾‹
    usage_example()